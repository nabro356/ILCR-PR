{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0HWslNrUJdla"
      },
      "outputs": [],
      "source": [
        "%pip install --upgrade transformers Keras-Preprocessing wandb pytorch-lightning sacremoses sentencepiece --quiet\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "from sklearn.metrics import (accuracy_score, f1_score, recall_score, precision_score, confusion_matrix)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rODCArHHPIB9"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "key=\"d75571bf9259088cd0a735d5f9e10de08e105a99\"\n",
        "wandb.login(key=key)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qzWRSDhVsSh4",
        "outputId": "b3a3b049-73aa-4f64-e151-286ae4707630"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SddyoAcaEH9w"
      },
      "outputs": [],
      "source": [
        "def model_training(model_type, batch_size, model_name, lr, epochs, file_name, MODEL_CLASSES, token_length):\n",
        "    df = pd.read_csv(file_name)\n",
        "    train_set = df[df['split'] == 'train']\n",
        "    test_set = df[df['split'] == 'test']\n",
        "    validation_set = df[df['split'] == 'val']\n",
        "\n",
        "    model_class, tokenizer_class, config_class = MODEL_CLASSES[model_type]\n",
        "    tokenizer = tokenizer_class.from_pretrained(model_name, from_tf=True)\n",
        "    def input_id_maker(dataf, tokenizer):\n",
        "        input_ids = []\n",
        "        lengths = []\n",
        "\n",
        "        for i in range(len(dataf)):\n",
        "            sen = dataf['text'].iloc[i]\n",
        "            sen = tokenizer.tokenize(sen)\n",
        "            # sen = tokenizer.tokenize(sen, add_prefix_space=True)\n",
        "            CLS = tokenizer.cls_token\n",
        "            SEP = tokenizer.sep_token\n",
        "            if (len(sen) > 510):\n",
        "                sen = sen[len(sen)-510:]\n",
        "\n",
        "            sen = [CLS] + sen + [SEP]\n",
        "            encoded_sent = tokenizer.convert_tokens_to_ids(sen)\n",
        "            input_ids.append(encoded_sent)\n",
        "            lengths.append(len(encoded_sent))\n",
        "\n",
        "        input_ids = pad_sequences(\n",
        "            input_ids, maxlen=token_length, value=0, dtype=\"long\", truncating=\"pre\", padding=\"post\")\n",
        "        return input_ids, lengths\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    train_input_ids, train_lengths = input_id_maker(train_set, tokenizer)\n",
        "    validation_input_ids, validation_lengths = input_id_maker(validation_set, tokenizer)\n",
        "\n",
        "    elapsed_time = time.time() - start_time\n",
        "\n",
        "    print(f\"Time taken for input_id_maker: {elapsed_time:.2f} seconds\\n\")\n",
        "\n",
        "    print(f\"Tokenization of data using {model_name} is done\\n\")\n",
        "\n",
        "    def att_masking(input_ids):\n",
        "        attention_masks = []\n",
        "        for sent in input_ids:\n",
        "            att_mask = [int(token_id > 0) for token_id in sent]\n",
        "            attention_masks.append(att_mask)\n",
        "        return attention_masks\n",
        "\n",
        "    train_attention_masks = att_masking(train_input_ids)\n",
        "    validation_attention_masks = att_masking(validation_input_ids)\n",
        "\n",
        "    train_labels = train_set['label'].to_numpy().astype('int')\n",
        "    validation_labels = validation_set['label'].to_numpy().astype('int')\n",
        "\n",
        "    print(f\"Masking of tokenizers is done\\n\")\n",
        "\n",
        "    train_inputs = train_input_ids\n",
        "    validation_inputs = validation_input_ids\n",
        "    train_masks = train_attention_masks\n",
        "    validation_masks = validation_attention_masks\n",
        "\n",
        "    train_inputs = torch.tensor(train_inputs)\n",
        "    train_labels = torch.tensor(train_labels)\n",
        "    train_masks = torch.tensor(train_masks)\n",
        "    validation_inputs = torch.tensor(validation_inputs)\n",
        "    validation_labels = torch.tensor(validation_labels)\n",
        "    validation_masks = torch.tensor(validation_masks)\n",
        "\n",
        "    train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "    train_sampler = RandomSampler(train_data)\n",
        "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size = batch_size)\n",
        "    validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "    validation_sampler = RandomSampler(validation_data)\n",
        "    validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size = batch_size)\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model_class.from_pretrained(model_name, num_labels=2, from_tf=True)\n",
        "    model.to(device)\n",
        "\n",
        "    max_grad_norm = 1.0\n",
        "    num_total_steps = len(train_dataloader)*epochs\n",
        "    num_warmup_steps = 1000\n",
        "    warmup_proportion = float(num_warmup_steps) / float(num_total_steps)  # 0.1\n",
        "    optimizer = AdamW(model.parameters(), lr=lr, correct_bias=True)\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = num_warmup_steps, num_training_steps = num_total_steps)\n",
        "\n",
        "    def flat_accuracy(preds, labels):\n",
        "        pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "        labels_flat = labels.flatten()\n",
        "        return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "    seed_val = 2212\n",
        "\n",
        "    np.random.seed(seed_val)\n",
        "    torch.manual_seed(seed_val)\n",
        "    torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "    checkpoint_folder = f\"CJPR/Transformers_GPU/{model_type}/{model_name}_L{lr}_E{epochs}_B{batch_size}\"\n",
        "\n",
        "    if not os.path.exists(checkpoint_folder):\n",
        "        os.makedirs(checkpoint_folder)\n",
        "\n",
        "    # os.makedirs(checkpoint_folder, exist_ok=True)\n",
        "\n",
        "    # def save_checkpoint(epoch, model, optimizer, scheduler, loss_values):\n",
        "    #     checkpoint = {\n",
        "    #         'epoch': epoch,\n",
        "    #         'model_state_dict': model.state_dict(),\n",
        "    #         'optimizer_state_dict': optimizer.state_dict(),\n",
        "    #         'scheduler_state_dict': scheduler.state_dict(),\n",
        "    #         'loss_values': loss_values,\n",
        "    #     }\n",
        "    #     checkpoint_file = os.path.join(checkpoint_folder, f'checkpoint_epoch_{epoch}.pt')\n",
        "    #     torch.save(checkpoint, checkpoint_file)\n",
        "\n",
        "    # def load_latest_checkpoint(model, optimizer, scheduler, loss_values):\n",
        "    #     checkpoint_files = [f for f in os.listdir(checkpoint_folder) if f.startswith('checkpoint_epoch_') and f.endswith('.pt')]\n",
        "    #     if checkpoint_files:\n",
        "    #         latest_checkpoint = max(checkpoint_files)\n",
        "    #         checkpoint_file = os.path.join(checkpoint_folder, latest_checkpoint)\n",
        "    #         checkpoint = torch.load(checkpoint_file)\n",
        "    #         model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    #         optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    #         scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "    #         loss_values.extend(checkpoint['loss_values'])\n",
        "    #         return checkpoint['epoch']\n",
        "    #     else:\n",
        "    #         return 0\n",
        "\n",
        "    loss_values = []\n",
        "    # checkpoint_epoch = load_latest_checkpoint(model, optimizer, scheduler, loss_values)\n",
        "\n",
        "    print(f\"Now Training for {model_name} is Started.......\\n\")\n",
        "\n",
        "    # for epoch_i in range(checkpoint_epoch, epochs):\n",
        "    for epoch_i in range(0, epochs):\n",
        "        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "        print('Training...')\n",
        "\n",
        "        t0 = time.time()\n",
        "        total_loss = 0\n",
        "\n",
        "        model.train()\n",
        "\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            if step % 100 == 0 and not step == 0:\n",
        "                print('  Batch {:>5,}  of  {:>5,}. '.format(step, len(train_dataloader)))\n",
        "\n",
        "            b_input_ids = batch[0].to(device)\n",
        "            b_input_mask = batch[1].to(device)\n",
        "            b_labels = batch[2].to(device)\n",
        "\n",
        "            model.zero_grad()\n",
        "\n",
        "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "            # outputs = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n",
        "\n",
        "            loss = outputs[0]\n",
        "            total_loss += loss.item()\n",
        "            loss.backward()\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "        avg_train_loss = total_loss / len(train_dataloader)\n",
        "        loss_values.append(avg_train_loss)\n",
        "\n",
        "        print(\"\")\n",
        "        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "\n",
        "        print(\"\")\n",
        "        print(\"Running Validation...\")\n",
        "\n",
        "        t0 = time.time()\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        eval_loss, eval_accuracy = 0, 0\n",
        "        nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "        for batch in validation_dataloader:\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "                # outputs = model(b_input_ids, attention_mask=b_input_mask)\n",
        "\n",
        "            logits = outputs[0].detach().cpu().numpy()\n",
        "            label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "            tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "            eval_accuracy += tmp_eval_accuracy\n",
        "            nb_eval_steps += 1\n",
        "\n",
        "        avg_eval_accuracy = eval_accuracy / nb_eval_steps\n",
        "\n",
        "        print(\"  Accuracy: {0:.2f}\".format(eval_accuracy / nb_eval_steps))\n",
        "        pred_flat = np.argmax(logits, axis=1).flatten()\n",
        "        labels_flat = label_ids.flatten()\n",
        "        macro_f1 = f1_score(pred_flat, labels_flat, average='macro')\n",
        "        micro_f1 = f1_score(pred_flat, labels_flat, average='micro')\n",
        "        accuracy = accuracy_score(pred_flat, labels_flat)\n",
        "        precision = precision_score(pred_flat, labels_flat)\n",
        "        recall = recall_score(pred_flat, labels_flat)\n",
        "        confusion = confusion_matrix(labels_flat, pred_flat)\n",
        "        epoch_metrics = {\n",
        "            'epoch': epoch_i,\n",
        "            'macro_f1': macro_f1,\n",
        "            'micro_f1': micro_f1,\n",
        "            \"flat_accuracy\":avg_eval_accuracy,\n",
        "            'accuracy': accuracy,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            \"Avg Train Loss\": avg_train_loss,\n",
        "            'confusion_matrix': confusion.tolist()\n",
        "        }\n",
        "\n",
        "        print(f\"epoch={epoch_i}, macro_f1: {macro_f1}, micro_f1: {micro_f1}, accuracy={accuracy}, precision: {precision}, recall: {recall}, loss={loss}\")\n",
        "\n",
        "        # Save the metrics to a JSON file for this epoch\n",
        "        epochs_folder = f\"{checkpoint_folder}/epochs\"\n",
        "        if not os.path.exists(epochs_folder):\n",
        "            os.makedirs(epochs_folder)\n",
        "\n",
        "        with open(f'{epochs_folder}/epoch{epoch_i}_metrics.json', 'w') as json_file:\n",
        "            json.dump(epoch_metrics, json_file, indent=4)\n",
        "        \n",
        "        print(f\"epoch_{epoch_i}_metrics.json saved to {epochs_folder}\\n\")\n",
        "\n",
        "        wandb.log({\"Flat Accuracy\":avg_eval_accuracy, \"Accuracy\":accuracy , \"Macro_f1\":macro_f1,\"Micro_f1\":micro_f1, \"Precision\":precision, \"Recall\":recall, \"Avg Train Loss\": avg_train_loss})\n",
        "\n",
        "        print(f\"epoch_{epoch_i} logging is done...\\n\")\n",
        "\n",
        "        # save_checkpoint(epoch_i, model, optimizer, scheduler, loss_values)\n",
        "    wandb.finish()\n",
        "\n",
        "    print(f\"Now Training for {model_name} is Completed.......\\n\")\n",
        "\n",
        "    print(\"Saving model to %s\\n\" % checkpoint_folder)\n",
        "\n",
        "    model_to_save = model.module if hasattr(model, 'module') else model\n",
        "    model_to_save.save_pretrained(checkpoint_folder)\n",
        "    tokenizer.save_pretrained(checkpoint_folder)\n",
        "\n",
        "    print(\"Model Saved to %s\\n\" % checkpoint_folder)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7Q9PNnweVuf"
      },
      "outputs": [],
      "source": [
        "from transformers import BertForSequenceClassification, BertTokenizer, BertConfig\n",
        "from transformers import RobertaForSequenceClassification, RobertaTokenizer, RobertaConfig\n",
        "from transformers import XLNetForSequenceClassification, XLNetTokenizer, XLNetConfig\n",
        "from transformers import XLMForSequenceClassification, XLMTokenizer, XLMConfig, XLMRobertaTokenizer\n",
        "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer, DistilBertConfig\n",
        "from transformers import DebertaV2ForSequenceClassification, DebertaV2Tokenizer, DebertaV2Config\n",
        "\n",
        "MODEL_CLASSES = {\n",
        "    'roberta': (RobertaForSequenceClassification, RobertaTokenizer, RobertaConfig),\n",
        "    'bert': (BertForSequenceClassification, BertTokenizer, BertConfig),\n",
        "    'xlnet': (XLNetForSequenceClassification, XLNetTokenizer, XLNetConfig),\n",
        "    'xlm': (XLMForSequenceClassification, XLMRobertaTokenizer, XLMConfig),\n",
        "    'distilbert': (DistilBertForSequenceClassification, DistilBertTokenizer, DistilBertConfig),\n",
        "    'deberta': (DebertaV2ForSequenceClassification, DebertaV2Tokenizer, DebertaV2Config)\n",
        "}\n",
        "\n",
        "model_type = 'xlm' \n",
        "\n",
        "token_length = [512, 1024, 2048]\n",
        "batch_size = [16, 32]\n",
        "lr = [1e-6, 1e-5, 1e-7]\n",
        "epochs = [10,15,20]\n",
        "file_name = 'data.csv'\n",
        "model_name = 'xlm-roberta-base'\n",
        "\n",
        "\n",
        "for token_length in token_length:\n",
        "    for batch_size in batch_size:\n",
        "        for lr in lr:\n",
        "            for epochs in epochs:\n",
        "                wandb.init(\n",
        "                    project=f\"{model_type}\",\n",
        "                    name=f\"{model_name}_L{lr}_B{batch_size}_E{epochs}\",\n",
        "\n",
        "                    config={\n",
        "                    \"architecture\": model_name,\n",
        "                    \"dataset\": \"ILDC\",\n",
        "                    \"learning_rate\": lr,\n",
        "                    \"epochs\": epochs,\n",
        "                    \"batch_size\": batch_size,\n",
        "                    \"token_length\": token_length,\n",
        "                    \"model_name\": model_name\n",
        "                    }\n",
        "                )\n",
        "                model_training(model_type, batch_size, model_name, lr, epochs, file_name, MODEL_CLASSES, token_length)\n",
        "                \n",
        "                wandb.finish()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "67f1a58412a5992903d8a569a5baa1f1514123756be715fd660b0e3603a8940d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
